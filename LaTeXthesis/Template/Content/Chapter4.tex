\chapter{Algorithms}
\label{ch:algorithms}



\section{Intuition}
\label{sec:intuition}

To solve this problem we have to evaluate all possible edge combinations. Even for greedy heuristics we need to limit the edge candidates. The algorithm considers nodes with a high expressed value. According to our model the smallest decrease is happening when we connect a value near zero and a relatively high value.
\\
\\
We will now see why this statement holds by examining how the expressed opinion changes with an addition in the Friedkin and Johnsen model. Consider an arbitrary example with two nodes inside a network. Node $a$ has $z_a = -0.02$ and node $b$ has $z_b = 0.5$. Also for this example we assume that $w_{ii}=w_{ij}=w_{ji}=1$. 
\\
\\
If we connect these two nodes with an edge and re calculate the expressed opinions both of the $z_i$ denominators will be increased by one. This emerges from the fact that both nodes will have an additional neighbour and that all weights equal with one. The numerator of the one node $a$ will be increased by a lot and the numerator of the node $b$ will be decreased by a small value.
\\
\\
The new $z_a$ will not change a lot because the big addition in the numerator will approach the +1 addition of the denominator. On the other hand the new $z_b$ will see a big change as the numerator had a small decrease thus creating a big decrease overall for this node. We can clearly see that only one of the two nodes will amount to a big decrease. 
\\
\\
Now consider a second example of two nodes node $c$ has $z_c = -0.8$ and node $d$ has $z_d = 0.9$. After the addition node $d$ will see a big decrease because we add two conflicting values that almost neutralise each other on the numerator but the addition of the +1 on the denominator stands still. On the other hand node $c$ will also see a big decrease for the same reason. With this type of connection both of the nodes have a significant decrease. In an optimal setting we would like $Z_c = Z_d$.
\\
\\
Now consider a setting that that $w_{ii} \neq w_{ij} \neq w_{ji}\neq 1$. The same intuition holds but now we want the expressed opinions together with their weights to neutralize each other. As before in an optimal setting we would like $w_{ij} * Z_v = w_{ij} * Z_u$. 

\section{Heuristics}
\label{sec:heuristics}

In this section we  consider a greedy algorithm and some heuristics for minimising $\pi(z)$. All the heuristics use the intuition that connecting the most extreme opinions of each community draw both of them into neutrality. The algorithms use two lists. One for each viewpoint sorted according to their opinion value. 
\\
\\
The Greedy algorithm computes the decrease in $\pi(z)$ and selects the edge with the largest decrease every time and comes in two versions. One that does not take into consideration the change that happens in the graph from the addition of the new edge while computing the next edge and one that does.
\\
\begin{figure}
  \begin{minipage}[b]{1\linewidth}
    \begin{algorithm}[H]
	\caption{Greedy minimization of $\pi(z)$}
	\label{alg:greedyAlgo}
	\begin{flushleft}
        		\textbf{INPUTS:} Graph $G$; $k$ number of edges to add;
		\vspace{6pt}
        		\textbf{OUTPUT:} Graph $G'$ with $k$ new edges that minimize the polarization index $\pi(z)$
	\end{flushleft}
	\begin{algorithmic}[1]
		\FOR {$i = 1:k \ $}
		\STATE$Decrease \leftarrow Empty List$;
			\FOR { each  edge in $|V| \times |V| \textbackslash E$}
				\STATE Compute the decrease of $\pi(z)$ if edge is added to the graph;
				\STATE Append the decrease on the $Decrease$ list;
			\ENDFOR
		\STATE Select the edge with the largest decrease from the $Decrease$ list.
		\STATE Add this edge to the graph.
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
\bigskip
  \end{minipage}
  \begin{minipage}[b]{1\linewidth}
     \begin{algorithm}[H]
	\caption{Greedy Batch}
	\label{alg:greedyBatch}
	\begin{flushleft}
        		\textbf{INPUTS:} Graph $G$; $k$ number of edges to add;
		$X$, $Y $, the set of vertices of each viewpoint $\epsilon$ [-1,0] and [0,1] respectively.\\
		\vspace{6pt}
        		\textbf{OUTPUT:} List of $k$ edges that minimize the polarization index $\pi(z)$
	\end{flushleft}
	\begin{algorithmic}[1]
		\STATE $EdgesToAdd \leftarrow Empty List;$
		\FOR { each  edge in $|V| \times |V| \textbackslash E$}
		\STATE Compute $\pi(z)$, the decrease if the edge $(u,v)$ is added;
		\STATE Append edge $(u,v)$ to EdgesToAdd;
		\ENDFOR
				\STATE $Sorted \leftarrow sort(EdgesToAdd)$ by the decrease of $\pi(z)$ by decreasing order;
		\STATE Return top $k$ from $Sorted$
	\end{algorithmic}
\end{algorithm}

  \end{minipage}%
\end{figure}
\vspace{10pt}

\clearpage


\noindent An improvement can be achieved by using the $\pi(z)$ as a condition to continue. There will be two sorted lists of the expressed opinions for each side of the argument.
\\
\\
While traversing the lists, in a descending manner, a list of $\pi(z)$ will be created. This list will contain the polarization of every node after adding an edge between all nodes of one of the viewpoints, one at a time, and the most extreme of the other. 
\\
\\
While traversing the lists, in a descending manner, the condition will allow us to skip edges that will have smaller decrease. This will done by breaking the second loop and reducing the number of computations needed. Even though the worst complexity is the same with ~\ref{alg:greedyWithout} it will not reach it in an average case.

\begin{algorithm}[htbp]
	\caption{Skip}
	\label{alg:SkipAlgo}
	\begin{flushleft}
        		\textbf{INPUTS:} Graph $G$; $k$ number of edges to add; Number of nodes $n$;
		$X$, $Y $, the sorted set of vertices according to polarization index of each viewpoint $\epsilon$ [-1,0] and [0,1] respectively.\\
		\vspace{6pt}
        		\textbf{OUTPUT:} List of $k$ edges that minimize the polarization index $\pi(z)$
	\end{flushleft}
	\begin{algorithmic}[1]
		\STATE $EdgesToAdd \leftarrow Empy List;$
		\FOR { each  edge in $|X| \times Y[0] $}
		\STATE Add edge to $G$, compute $\pi(z)$ and append it in an array;
		\STATE Remove this edge from $G$
		\ENDFOR
		
		\FOR { each  edge in $|X| \times |Y| $}		
		\STATE Compute $\pi(z)$, the decrease if the edge is added;
		\IF  {this $\pi(z)$ > $\pi(z)$ of the next edge in the sorted list}
			\STATE continue; // skip edges
		\ENDIF
		\STATE Append edge $(u,v)$ to EdgesToAdd;
		\IF  {$SizeOf(EdgesToAdd) = k$}
			\STATE return EdgesToAdd;
		\ENDIF
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
\clearpage

\noindent We can further improve  Algorithm ~\ref{alg:SkipAlgo} by removing the need to compute the initial list for comparison. 
\\
\\
Instead the $\pi(z)$ a function of distance can be used as condition to continue. The same holds for the complexity.
\\
\\

\begin{algorithm}[htbp]
	\caption{Expressed Distance}
	\label{alg:ExpressedAlgo}
	\begin{flushleft}
        		\textbf{INPUTS:} Graph $G$; $k$ number of edges to add; Number of nodes $n$;
		$X$, $Y $, the sorted set of vertices according to polarization index of each viewpoint $\epsilon$ [-1,0] and [0,1] respectively.\\
		\vspace{6pt}
        		\textbf{OUTPUT:} List of $k$ edges that minimize the polarization index $\pi(z)$
	\end{flushleft}
	\begin{algorithmic}[1]
		\STATE $EdgesToAdd \leftarrow Empty List;$
		\FOR{$u$ in $X$}
			\FOR{$v$ in $Y$}
			\STATE $current$ := distance of $z$ values between $u$ and $v$;
			\STATE $skip$ := distance of $z$ values between next node in $X$ and $Y[0]$;
			\IF {$current \leqslant skip$}
				\STATE continue;
			\ENDIF
		\STATE Append edge $(u,v)$ to EdgesToAdd;
		\IF  {$SizeOf(EdgesToAdd) = k$}
			\STATE return EdgesToAdd;
		\ENDIF
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\clearpage
\noindent We consider two more heuristics. These two iterate only over the missing edges of the graph. The first computes the distance of the expressed values of the edges and keeps the smallest ones while the second makes a multiplication with these values and keeps the smallest negative ones.
	\begin{algorithm}[H]
	\caption{Distance Missing Edges}
	\label{alg:expreDisMiss}
	\begin{flushleft}
        		\textbf{INPUTS:} Graph $G$; $k$ number of edges to add;
		$X$, $Y $, the set of vertices of each viewpoint $\epsilon$ [-1,0] and [0,1] respectively.\\
		\vspace{6pt}
        		\textbf{OUTPUT:} List of $k$ edges that minimize the polarization index $\pi(z)$
	\end{flushleft}
	\begin{algorithmic}[1]
		\STATE $EdgesToAdd \leftarrow Empty List;$
		\FOR { each  edge in $|V| \times |V| \textbackslash E$}
		\STATE Append to $EddgesToAdd$ the distance of $z$ value between $u$ and $v$;
		\ENDFOR
		\STATE $Sorted \leftarrow sort(EdgesToAdd)$ by decreasing order;
		\STATE Return edges that correspond to the top $k$ from $Sorted$
	\end{algorithmic}
\end{algorithm}
 \begin{algorithm}[H]
	\caption{Multiplication Missing Edges}
	\label{alg:expreMulMiss}
	\begin{flushleft}
        		\textbf{INPUTS:} Graph $G$; $k$ number of edges to add;
		$X$, $Y $, the set of vertices of each viewpoint $\epsilon$ [-1,0] and [0,1] respectively.\\
		\vspace{6pt}
        		\textbf{OUTPUT:} List of $k$ edges that minimize the polarization index $\pi(z)$
	\end{flushleft}
	\begin{algorithmic}[1]
		\STATE $EdgesToAdd \leftarrow Empty List;$
		\FOR { each  edge in $|V| \times |V| \textbackslash E$}
		\STATE Append to $EddgesToAdd$ the multiplication of $z$ values between $u$ and $v$;
		\ENDFOR
		\STATE $Sorted \leftarrow sort(EdgesToAdd)$ by increasing order;
		\STATE Return edges that correspond to the top $k$ from $Sorted$
	\end{algorithmic}

\end{algorithm}

\clearpage


\section{Link Prediction}
\label{sec:linkpred}

Link prediction is the problem of predicting the existence of a link between two entities in a network in the future. In our setting, social media networks, the entity represents a person. For example the "People you may know" section on Facebook.
\\
\\
Link prediction algorithms are based on how similar two different nodes are, what features they have in common, how are they connected to the rest of the network or how many other nodes are connected to a single node. 
\\
\\
Link prediction is also used in recommendation systems and  information retrieval. We will use the similarity measures mentioned in \ref{sec:simMeasures} as features for our machine learning model.

\subsection{Graph Embeddings}
\label{sec:embeddings}

A graph embedding is the transformation of the properties of the graphs to a vector or a set of vectors. The embedding will capture the topology of the graph and will consider the relationship between nodes. The embedding will be used to make predictions on the graph.
\\
\\
\noindent Machine learning on graphs is limited while vector spaces have a much bigger toolset available. In essence embeddings are compressed representations in a vector that has a smaller dimension.

\subsection{Word2Vec}

At first we have to define Word2Vec. Suppose we have a sentence of words. For a certain task a simple neural network with a single hidden layer is created. The trained neural network is not actually used for the task that we trained it on. The goals is to learn the weights of the hidden layer. These weights represent the "word vectors".
\\
\\
After giving the neural network a word in the middle of a sentence, it is trained to look for nearby words and pick a random one. The network is going to give the probability for every word in our vocabulary of being inside a window size we set.
\\
\\
The output probabilities are going to relate to how likely it is to find each vocabulary word nearby our input word. The neural network is trained by feeding it word pairs found in training examples.
\\
\\
The hidden layer of this model is operating as a lookup table. The output of the hidden layer is just the “word vector” for the input word.
\\
\\
The word vector will then get fed to the output layer. The output layer is a softmax regression classifier. Each output neuron will produce an output between 0 and 1, and the sum of all these output values will add up to 1.
\\
\\
If two different words have very similar context then our model needs to output very similar results for these two words.


\subsection{DeepWalk}

After defining Word2Vec we can use its logic in graphs. DeepWalk uses random walks to produce embeddings. The random walk starts in a selected node and then moves to a random neighbour from a current node with certain number of steps. The method consists of three steps.

 \begin{itemize}
  \item Sampling: A graph is sampled with random walks. Authors show that it is sufficient to perform from 32 to 64 random walks from each node. 
  
    
  \item Training skip-gram: Random walks are comparable to sentences in word2vec approach. The skip-gram network accepts a node from the random walk a vector as an input and maximizes the probability for predicting neighbour nodes. 
  
  \item Computing embeddings: Embedding is the output of a hidden layer of the network. The DeepWalk computes embedding for each node in the graph.
   
  \end{itemize}

\noindent  DeepWalk method performs the walks randomly and that means that embeddings do not preserve the local neighbourhood. Node2vec approach fixes that.

\subsection{Node2Vec}

Node2vec is a modification of DeepWalk with a small difference in  the implementation of random walks. There are two parameters introduced, $P$ and $Q$. 
\\
\\
Parameter $Q$ defines how probable is that the random walk will explore the undiscovered part of the graph, while parameter $P$ defines how probable is that the random walk will return to the previous node and retain a locality.

\subsection{Methodology}
\label{sec:methodology}

Our objective is to predict whether there would be a link between 2 unconnected nodes. At first we will extract the pairs of nodes that don't have a link between them.
\\
\\
The next step is to hide some edges from the given graph. This is needed for preparing a training dataset. As a social network grows new edges are introduced. The machine learning model needs how the graph evolved.The graph with the hidden edges is the graph $G$ at time $t$ and our current dataset is the graph $G$ at time $t+n$.
\\
\\
While removing links or edges, we should avoid removing any edge that may produce non connected nodes or networks. The next step is to create features for all the unconnected node pairs including the ones for which we have hid. The removed edges will be labeled as $1$ (positive samples) and the unconnected node pairs as $0$ (negative samples).
\\
\\
After the labelling we will use the node2vec algorithm to extract node features from the graph. For computing the features of an edge we can add up the features of the nodes of that pair. These features will be trained with a logistic regression model.

\clearpage



